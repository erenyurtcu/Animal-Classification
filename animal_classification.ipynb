{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":2333429,"sourceType":"datasetVersion","datasetId":1408532}],"dockerImageVersionId":30822,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Dataset Preparation**\nThis script prepares the dataset for training by performing the following operations:\n1. **Dataset Directory and Class Selection:**\n    * Defines the dataset directory, output directory, and the processed directory.\n    * Specifies the selected animal classes to include in the project.\n\n2. **Image Limits and Preprocessing Settings:**\n    * Limits the maximum number of images per class to 650.\n    * Sets the desired image size for resizing (e.g., 128x128).\n\n3. **Output Directories Setup:**\n    * Clears existing directories for the output and processed data, ensuring a clean start.\n    * Recreates these directories for saving processed images.\n\n4. **Image Copying and Processing:**\n    * Iterates through the selected animal classes.\n    * Copies a subset of images (up to the specified limit) for each class to the output directory.\n    * Resizes and normalizes images before saving them in the processed directory.","metadata":{}},{"cell_type":"code","source":"# Import necessary libraries\nimport cv2\nimport numpy as np\nimport os\nimport shutil\nfrom pathlib import Path\n\n# Dataset directory (Adjust based on your file structure)\ndata_dir = \"/kaggle/input/animals-with-attributes-2/Animals_with_Attributes2/JPEGImages\"\noutput_dir = \"/kaggle/working/selected_animals_dataset\"\nprocessed_dir = \"/kaggle/working/processed_dataset\"\n\n# Selected classes\nselected_classes = [\n    \"collie\", \"dolphin\", \"elephant\", \"fox\", \"moose\", \n    \"rabbit\", \"sheep\", \"squirrel\", \"giant+panda\", \"polar+bear\"\n]\n\n# Maximum number of images to keep per class\nmax_images_per_class = 650\n\n# Desired image size\nimage_size = (128, 128)  # Example size, can be adjusted based on model input\n\n# Clear and recreate the output and processed directories\nif os.path.exists(output_dir):\n    shutil.rmtree(output_dir)\nos.makedirs(output_dir)\n\nif os.path.exists(processed_dir):\n    shutil.rmtree(processed_dir)\nos.makedirs(processed_dir)\n\n# Copy selected classes and process images\nfor animal_class in selected_classes:\n    class_path = os.path.join(data_dir, animal_class)\n    if os.path.exists(class_path):\n        images = sorted(os.listdir(class_path))[:max_images_per_class]\n        output_class_path = os.path.join(output_dir, animal_class)\n        processed_class_path = os.path.join(processed_dir, animal_class)\n        os.makedirs(output_class_path, exist_ok=True)\n        os.makedirs(processed_class_path, exist_ok=True)\n\n        for image in images:\n            source_path = os.path.join(class_path, image)\n            target_path = os.path.join(output_class_path, image)\n            shutil.copy2(source_path, target_path)\n\n            # Load image, resize and normalize\n            img = cv2.imread(source_path)\n            if img is not None:\n                img_resized = cv2.resize(img, image_size)\n                img_normalized = img_resized / 255.0\n                processed_image_path = os.path.join(processed_class_path, image)\n                cv2.imwrite(processed_image_path, (img_normalized * 255).astype(np.uint8))\n\nprint(f\"Selected classes and the first {max_images_per_class} images have been copied to '{output_dir}', and processed images saved to '{processed_dir}'.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T14:19:43.285276Z","iopub.execute_input":"2024-12-20T14:19:43.285586Z","iopub.status.idle":"2024-12-20T14:21:11.232220Z","shell.execute_reply.started":"2024-12-20T14:19:43.285560Z","shell.execute_reply":"2024-12-20T14:21:11.230824Z"}},"outputs":[{"name":"stdout","text":"Selected classes and the first 650 images have been copied to '/kaggle/working/selected_animals_dataset', and processed images saved to '/kaggle/working/processed_dataset'.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"# **Train and Test Data Preparation**\nThis script processes the dataset for training and testing by performing the following operations:\n1. **Dataset and Label Initialization:**\n    * Defines the processed dataset directory.\n    * Lists the selected animal classes and maps them to numerical labels.\n\n2. **Data Loading and Normalization:**\n    * Iterates through the processed images for each selected class.\n    * Loads and normalizes the pixel values to the range [0, 1].\n\n3. **Data Conversion:**\n    * Converts the lists of images and labels into NumPy arrays for model compatibility.\n\n4. **Data Splitting:**\n    * Splits the dataset into training and testing sets with a 70-30 split.\n    * Ensures reproducibility using a fixed random seed.","metadata":{}},{"cell_type":"code","source":"# Import necessary libraries\nimport os\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\n\n# Processed dataset directory\nprocessed_dir = \"/kaggle/working/processed_dataset\"\n\n# Selected classes\nselected_classes = [\n    \"collie\", \"dolphin\", \"elephant\", \"fox\", \"moose\", \n    \"rabbit\", \"sheep\", \"squirrel\", \"giant+panda\", \"polar+bear\"\n]\n\n# Prepare data and labels\nX = []  # Images\ny = []  # Labels\nclass_mapping = {cls: idx for idx, cls in enumerate(selected_classes)}\n\n# Load processed images and labels\nfor animal_class in selected_classes:\n    class_path = os.path.join(processed_dir, animal_class)\n    if os.path.exists(class_path):\n        images = sorted(os.listdir(class_path))\n\n        for image in images:\n            source_path = os.path.join(class_path, image)\n\n            # Load image and normalize (already resized)\n            img = cv2.imread(source_path)\n            if img is not None:\n                img_normalized = img / 255.0\n                X.append(img_normalized)\n                y.append(class_mapping[animal_class])\n\n# Convert lists to numpy arrays\nX = np.array(X)\ny = np.array(y)\n\n# Split the data into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\nprint(f\"Data has been split into training and testing sets.\\nTrain size: {len(X_train)}, Test size: {len(X_test)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T14:21:23.796884Z","iopub.execute_input":"2024-12-20T14:21:23.797272Z","iopub.status.idle":"2024-12-20T14:21:31.476292Z","shell.execute_reply.started":"2024-12-20T14:21:23.797239Z","shell.execute_reply":"2024-12-20T14:21:31.475088Z"}},"outputs":[{"name":"stdout","text":"Data has been split into training and testing sets.\nTrain size: 4550, Test size: 1950\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"# **Data Augmentation for Training Set**\n\n1. **Data Augmentation Initialization:**\n    * Creates an instance of ImageDataGenerator with the following augmentation options:\n        * **Rotation:** Random rotations within ±20 degrees.\n        * **Horizontal and Vertical Shifts:** Randomly shifts images horizontally and vertically by 20% of the total width or height.\n        * **Shear Transformations:** Applies random shearing transformations.\n        * **Zoom:** Randomly zooms into the images by 20%.\n        * **Horizontal Flip:** Randomly flips images horizontally.\n        * **Fill Mode:** Specifies how to fill empty pixels created after transformations.\n\n2. **Augmentation Application::**\n   * Augments a batch of training images (X_train) and corresponding labels (y_train) using a batch size of 32.\n\n3. **Output:**\n   * Confirms that data augmentation has been successfully applied to the training set.","metadata":{}},{"cell_type":"code","source":"# Import necessary libraries\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\n\n# Apply data augmentation to the training set\naugmentation = ImageDataGenerator(\n    rotation_range=20,  # Random rotation\n    width_shift_range=0.2,  # Random horizontal shift\n    height_shift_range=0.2,  # Random vertical shift\n    shear_range=0.2,  # Shear transformations\n    zoom_range=0.2,  # Random zoom\n    horizontal_flip=True,  # Random horizontal flip\n    fill_mode='nearest'  # Filling strategy for empty pixels\n)\n\n# Example: Augment a batch of images (X_train)\naugmented_data = augmentation.flow(X_train, y_train, batch_size=32)\n\nprint(\"Data augmentation has been applied to the training set.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T14:21:56.768071Z","iopub.execute_input":"2024-12-20T14:21:56.768555Z","iopub.status.idle":"2024-12-20T14:22:08.228134Z","shell.execute_reply.started":"2024-12-20T14:21:56.768521Z","shell.execute_reply":"2024-12-20T14:22:08.226950Z"}},"outputs":[{"name":"stdout","text":"Data augmentation has been applied to the training set.\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"# **Image Manipulation and White Balance Correction**\n\nThis script performs two key operations on image datasets:\n\n1. **Manipulating Images**: \n   - Adjusts brightness and contrast to simulate various lighting conditions.\n   - Saves the manipulated images to a specified output directory.\n\n2. **Applying White Balance Correction**:\n   - Implements the Gray World assumption to normalize colors across images.\n   - Adjusts the pixel values based on the average intensities of RGB channels.\n\n## **Image Manipulation**\n\n### **Overview**\nThe function `get_manipulated_images`:\n- Adjusts brightness and contrast for all images in the specified input directory.\n- Saves the manipulated images to a designated output directory.\n\n### **Key Parameters**\n- **Brightness Factor**: Controls the overall brightness of the images.\n- **Contrast Factor**: Adjusts the difference between dark and light areas in the image.","metadata":{}},{"cell_type":"code","source":"# Import necessary libraries\nimport cv2\nimport os\nimport numpy as np\n\n# Function to manipulate images\ndef get_manipulated_images(input_dir, output_dir, brightness_factor=1.5, contrast_factor=1.2):\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n    \n    for class_name in os.listdir(input_dir):\n        class_path = os.path.join(input_dir, class_name)\n        output_class_path = os.path.join(output_dir, class_name)\n        \n        if not os.path.exists(output_class_path):\n            os.makedirs(output_class_path)\n        \n        for img_name in os.listdir(class_path):\n            img_path = os.path.join(class_path, img_name)\n            img = cv2.imread(img_path)\n            \n            if img is not None:\n                # Apply brightness and contrast adjustments\n                manipulated_img = cv2.convertScaleAbs(img, alpha=contrast_factor, beta=brightness_factor * 50)\n                \n                # Save manipulated image\n                save_path = os.path.join(output_class_path, img_name)\n                cv2.imwrite(save_path, manipulated_img)\n    \n    print(f\"Manipulated images saved to {output_dir}\")\n\n# Paths\ninput_dir = \"/kaggle/working/processed_dataset\"\nmanipulated_dir = \"/kaggle/working/manipulated_images\"\n\n# Generate manipulated images\nget_manipulated_images(input_dir, manipulated_dir)\n\n\n# Gray World Assumption implementation\ndef get_wb_images(input_dir, output_dir):\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n\n    for class_name in os.listdir(input_dir):\n        class_path = os.path.join(input_dir, class_name)\n        output_class_path = os.path.join(output_dir, class_name)\n\n        if not os.path.exists(output_class_path):\n            os.makedirs(output_class_path)\n\n        for img_name in os.listdir(class_path):\n            img_path = os.path.join(class_path, img_name)\n            img = cv2.imread(img_path)\n\n            if img is not None:\n                # Apply Gray World Assumption for white balance correction\n                avg_b = np.mean(img[:, :, 0])\n                avg_g = np.mean(img[:, :, 1])\n                avg_r = np.mean(img[:, :, 2])\n                avg_gray = (avg_b + avg_g + avg_r) / 3\n\n                img[:, :, 0] = np.clip(img[:, :, 0] * (avg_gray / avg_b), 0, 255)\n                img[:, :, 1] = np.clip(img[:, :, 1] * (avg_gray / avg_g), 0, 255)\n                img[:, :, 2] = np.clip(img[:, :, 2] * (avg_gray / avg_r), 0, 255)\n\n                # Save the corrected image\n                save_path = os.path.join(output_class_path, img_name)\n                cv2.imwrite(save_path, img.astype(np.uint8))\n\n    print(f\"White-balanced images saved to {output_dir}\")\n\n# Paths\nwb_output_dir = \"/kaggle/working/wb_corrected_images\"\n\n# Apply Gray World Assumption\nget_wb_images(manipulated_dir, wb_output_dir)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T14:31:08.382872Z","iopub.execute_input":"2024-12-20T14:31:08.383552Z","iopub.status.idle":"2024-12-20T14:31:19.691127Z","shell.execute_reply.started":"2024-12-20T14:31:08.383509Z","shell.execute_reply":"2024-12-20T14:31:19.690039Z"}},"outputs":[{"name":"stdout","text":"Manipulated images saved to /kaggle/working/manipulated_images\nWhite-balanced images saved to /kaggle/working/wb_corrected_images\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"# **Loading Manipulated and White-Balanced Datasets**\n\nThis section demonstrates how to load manipulated and white-balanced datasets for further use in training or evaluation.","metadata":{}},{"cell_type":"code","source":"# Paths\nmanipulated_dir = \"/kaggle/working/manipulated_images\"  # Manipulated dataset directory\nwb_output_dir = \"/kaggle/working/wb_corrected_images\"  # White-balanced dataset directory\n\n# Load manipulated dataset\nX_manipulated_train, y_manipulated_train = load_images_from_directory(manipulated_dir, selected_classes)\nprint(f\"Manipulated dataset loaded: {X_manipulated_train.shape}, {y_manipulated_train.shape}\")\n\n# Load white-balanced dataset\nX_wb_train, y_wb_train = load_images_from_directory(wb_output_dir, selected_classes)\nprint(f\"White-balanced dataset loaded: {X_wb_train.shape}, {y_wb_train.shape}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T14:32:34.152289Z","iopub.execute_input":"2024-12-20T14:32:34.152683Z","iopub.status.idle":"2024-12-20T14:32:45.821158Z","shell.execute_reply.started":"2024-12-20T14:32:34.152641Z","shell.execute_reply":"2024-12-20T14:32:45.820240Z"}},"outputs":[{"name":"stdout","text":"Manipulated dataset loaded: (6500, 128, 128, 3), (6500,)\nWhite-balanced dataset loaded: (6500, 128, 128, 3), (6500,)\n","output_type":"stream"}],"execution_count":10},{"cell_type":"markdown","source":"# **Combining Datasets and Training the Model**\n\nThis section demonstrates the process of combining datasets (original, manipulated, and white-balanced) and training a CNN model on the combined data.\n\n---\n\n## **Combining Datasets**\n\n### **Overview**\n- Original, manipulated, and white-balanced datasets are merged into a single dataset.\n- Ensures the model learns from diverse data distributions.\n\n### **Code**\n```python\n# Combine Original, Manipulated, and White-Balanced Datasets\nX_combined_train = np.concatenate([X_train, X_manipulated_train, X_wb_train], axis=0)\ny_combined_train = np.concatenate([y_train, y_manipulated_train, y_wb_train], axis=0)\n","metadata":{}},{"cell_type":"code","source":"# Import necessary libraries\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, Input, LeakyReLU, BatchNormalization\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.regularizers import l2\nfrom sklearn.utils import shuffle\nimport numpy as np\n\n# Combine Original, Manipulated, and White-Balanced Datasets\nX_combined_train = np.concatenate([X_train, X_manipulated_train, X_wb_train], axis=0)\ny_combined_train = np.concatenate([y_train, y_manipulated_train, y_wb_train], axis=0)\n\n# Convert labels to one-hot encoding\ny_combined_train_one_hot = to_categorical(y_combined_train, num_classes=10)\ny_test_one_hot = to_categorical(y_test, num_classes=10)\n\n# Shuffle the combined dataset\nX_combined_train, y_combined_train_one_hot = shuffle(X_combined_train, y_combined_train_one_hot, random_state=42)\n\n# Define CNN model\nmodel = Sequential([\n    Input(shape=(128, 128, 3)),\n    Conv2D(32, (3, 3)),\n    LeakyReLU(negative_slope=0.1),\n    BatchNormalization(),\n    MaxPooling2D((2, 2)),\n\n    Conv2D(64, (3, 3)),\n    LeakyReLU(negative_slope=0.1),\n    BatchNormalization(),\n    MaxPooling2D((2, 2)),\n\n    Conv2D(128, (3, 3)),\n    LeakyReLU(negative_slope=0.1),\n    BatchNormalization(),\n    MaxPooling2D((2, 2)),\n\n    Conv2D(256, (3, 3)),\n    LeakyReLU(negative_slope=0.1),\n    BatchNormalization(),\n    MaxPooling2D((2, 2)),\n\n    Flatten(),\n    Dense(128, kernel_regularizer=l2(0.02)),\n    LeakyReLU(negative_slope=0.1),\n    Dropout(0.7),\n    Dense(10, activation='softmax')  # 10 classes\n])\n\n# Compile the model\nmodel.compile(\n    optimizer=Adam(learning_rate=0.0001),\n    loss='categorical_crossentropy',\n    metrics=['accuracy']\n)\n\n# Train the model\nhistory = model.fit(\n    X_combined_train, y_combined_train_one_hot,\n    epochs=20,  # Increased epochs\n    batch_size=32,\n    validation_data=(X_test, y_test_one_hot)\n)\n\n# Model summary\nmodel.summary()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T14:32:47.105500Z","iopub.execute_input":"2024-12-20T14:32:47.105898Z","iopub.status.idle":"2024-12-20T17:34:35.745720Z","shell.execute_reply.started":"2024-12-20T14:32:47.105843Z","shell.execute_reply":"2024-12-20T17:34:35.743038Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/20\n\u001b[1m549/549\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m548s\u001b[0m 991ms/step - accuracy: 0.2678 - loss: 7.2221 - val_accuracy: 0.4728 - val_loss: 5.4394\nEpoch 2/20\n\u001b[1m549/549\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m556s\u001b[0m 981ms/step - accuracy: 0.4602 - loss: 5.2359 - val_accuracy: 0.6313 - val_loss: 4.0120\nEpoch 3/20\n\u001b[1m549/549\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m534s\u001b[0m 972ms/step - accuracy: 0.5506 - loss: 4.0436 - val_accuracy: 0.6703 - val_loss: 3.1654\nEpoch 4/20\n\u001b[1m549/549\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m567s\u001b[0m 981ms/step - accuracy: 0.6025 - loss: 3.1812 - val_accuracy: 0.7031 - val_loss: 2.5174\nEpoch 5/20\n\u001b[1m549/549\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m565s\u001b[0m 986ms/step - accuracy: 0.6756 - loss: 2.4854 - val_accuracy: 0.7144 - val_loss: 2.1081\nEpoch 6/20\n\u001b[1m549/549\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m534s\u001b[0m 973ms/step - accuracy: 0.7256 - loss: 1.9903 - val_accuracy: 0.8062 - val_loss: 1.5740\nEpoch 7/20\n\u001b[1m549/549\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m534s\u001b[0m 972ms/step - accuracy: 0.7822 - loss: 1.6121 - val_accuracy: 0.8487 - val_loss: 1.3182\nEpoch 8/20\n\u001b[1m549/549\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m548s\u001b[0m 998ms/step - accuracy: 0.8291 - loss: 1.3027 - val_accuracy: 0.8503 - val_loss: 1.1331\nEpoch 9/20\n\u001b[1m549/549\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m527s\u001b[0m 961ms/step - accuracy: 0.8711 - loss: 1.0795 - val_accuracy: 0.8656 - val_loss: 1.0196\nEpoch 10/20\n\u001b[1m549/549\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m531s\u001b[0m 966ms/step - accuracy: 0.8966 - loss: 0.9152 - val_accuracy: 0.9128 - val_loss: 0.8201\nEpoch 11/20\n\u001b[1m549/549\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m535s\u001b[0m 974ms/step - accuracy: 0.9188 - loss: 0.7835 - val_accuracy: 0.9062 - val_loss: 0.7655\nEpoch 12/20\n\u001b[1m549/549\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m567s\u001b[0m 983ms/step - accuracy: 0.9294 - loss: 0.6998 - val_accuracy: 0.9056 - val_loss: 0.7211\nEpoch 13/20\n\u001b[1m549/549\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m537s\u001b[0m 977ms/step - accuracy: 0.9444 - loss: 0.6123 - val_accuracy: 0.9210 - val_loss: 0.6426\nEpoch 14/20\n\u001b[1m549/549\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m537s\u001b[0m 977ms/step - accuracy: 0.9532 - loss: 0.5499 - val_accuracy: 0.9533 - val_loss: 0.5353\nEpoch 15/20\n\u001b[1m549/549\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m550s\u001b[0m 1s/step - accuracy: 0.9501 - loss: 0.5264 - val_accuracy: 0.9195 - val_loss: 0.5986\nEpoch 16/20\n\u001b[1m549/549\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m534s\u001b[0m 972ms/step - accuracy: 0.9553 - loss: 0.4968 - val_accuracy: 0.9472 - val_loss: 0.5189\nEpoch 17/20\n\u001b[1m549/549\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m567s\u001b[0m 981ms/step - accuracy: 0.9668 - loss: 0.4538 - val_accuracy: 0.9436 - val_loss: 0.5031\nEpoch 18/20\n\u001b[1m549/549\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m532s\u001b[0m 970ms/step - accuracy: 0.9689 - loss: 0.4276 - val_accuracy: 0.9451 - val_loss: 0.4911\nEpoch 19/20\n\u001b[1m549/549\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m536s\u001b[0m 976ms/step - accuracy: 0.9668 - loss: 0.4287 - val_accuracy: 0.9456 - val_loss: 0.4801\nEpoch 20/20\n\u001b[1m549/549\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m543s\u001b[0m 989ms/step - accuracy: 0.9682 - loss: 0.4174 - val_accuracy: 0.9492 - val_loss: 0.4589\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"sequential\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m126\u001b[0m, \u001b[38;5;34m126\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │             \u001b[38;5;34m896\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ leaky_re_lu (\u001b[38;5;33mLeakyReLU\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m126\u001b[0m, \u001b[38;5;34m126\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │               \u001b[38;5;34m0\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ batch_normalization                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m126\u001b[0m, \u001b[38;5;34m126\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │             \u001b[38;5;34m128\u001b[0m │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ max_pooling2d (\u001b[38;5;33mMaxPooling2D\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m63\u001b[0m, \u001b[38;5;34m63\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │               \u001b[38;5;34m0\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m61\u001b[0m, \u001b[38;5;34m61\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │          \u001b[38;5;34m18,496\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ leaky_re_lu_1 (\u001b[38;5;33mLeakyReLU\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m61\u001b[0m, \u001b[38;5;34m61\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │               \u001b[38;5;34m0\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ batch_normalization_1                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m61\u001b[0m, \u001b[38;5;34m61\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │             \u001b[38;5;34m256\u001b[0m │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ max_pooling2d_1 (\u001b[38;5;33mMaxPooling2D\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │               \u001b[38;5;34m0\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ conv2d_2 (\u001b[38;5;33mConv2D\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │          \u001b[38;5;34m73,856\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ leaky_re_lu_2 (\u001b[38;5;33mLeakyReLU\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │               \u001b[38;5;34m0\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ batch_normalization_2                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │             \u001b[38;5;34m512\u001b[0m │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ max_pooling2d_2 (\u001b[38;5;33mMaxPooling2D\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │               \u001b[38;5;34m0\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ conv2d_3 (\u001b[38;5;33mConv2D\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m256\u001b[0m)         │         \u001b[38;5;34m295,168\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ leaky_re_lu_3 (\u001b[38;5;33mLeakyReLU\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m256\u001b[0m)         │               \u001b[38;5;34m0\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ batch_normalization_3                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m256\u001b[0m)         │           \u001b[38;5;34m1,024\u001b[0m │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ max_pooling2d_3 (\u001b[38;5;33mMaxPooling2D\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m256\u001b[0m)           │               \u001b[38;5;34m0\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ flatten (\u001b[38;5;33mFlatten\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m9216\u001b[0m)                │               \u001b[38;5;34m0\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │       \u001b[38;5;34m1,179,776\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ leaky_re_lu_4 (\u001b[38;5;33mLeakyReLU\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dropout (\u001b[38;5;33mDropout\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)                  │           \u001b[38;5;34m1,290\u001b[0m │\n└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">126</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">126</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ leaky_re_lu (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">126</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">126</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ batch_normalization                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">126</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">126</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ max_pooling2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">63</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">63</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">61</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">61</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │          <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ leaky_re_lu_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">61</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">61</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ batch_normalization_1                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">61</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">61</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ max_pooling2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ conv2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ leaky_re_lu_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ batch_normalization_2                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ max_pooling2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ conv2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">295,168</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ leaky_re_lu_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)         │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ batch_normalization_3                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)         │           <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ max_pooling2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)           │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">9216</span>)                │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │       <span style=\"color: #00af00; text-decoration-color: #00af00\">1,179,776</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ leaky_re_lu_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)                  │           <span style=\"color: #00af00; text-decoration-color: #00af00\">1,290</span> │\n└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m4,712,288\u001b[0m (17.98 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,712,288</span> (17.98 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,570,442\u001b[0m (5.99 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,570,442</span> (5.99 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m960\u001b[0m (3.75 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">960</span> (3.75 KB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m3,140,886\u001b[0m (11.98 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,140,886</span> (11.98 MB)\n</pre>\n"},"metadata":{}}],"execution_count":11},{"cell_type":"markdown","source":"# **Evaluating the Model on Manipulated Test Set**\n\n## **Overview**\nThis section evaluates the model's performance on the manipulated test set, which contains images with brightness and contrast adjustments. The goal is to assess how well the model generalizes to altered lighting conditions.\n\n\n## **Steps**\n\n### **1. Load Manipulated Test Set**\n- The manipulated dataset is loaded from the specified directory.\n- Images and labels are extracted.\n\n#### **Code**\n```python\nmanipulated_dir = \"/kaggle/working/manipulated_images\"  # Adjust path as needed\n\n# Load manipulated test images and labels\nX_manipulated_test, y_manipulated_test = load_images_from_directory(manipulated_dir, selected_classes)\nprint(f\"Manipulated Test Set Loaded: {X_manipulated_test.shape}, {y_manipulated_test.shape}\")","metadata":{}},{"cell_type":"code","source":"# Load manipulated test set\nmanipulated_dir = \"/kaggle/working/manipulated_images\"  # Adjust path as needed\n\n# Load manipulated test images and labels\nX_manipulated_test, y_manipulated_test = load_images_from_directory(manipulated_dir, selected_classes)\n\nprint(f\"Manipulated Test Set Loaded: {X_manipulated_test.shape}, {y_manipulated_test.shape}\")\n\n# Convert manipulated test labels to one-hot encoding\ny_manipulated_test_one_hot = to_categorical(y_manipulated_test, num_classes=10)\n\n# Evaluate the model on manipulated test set\nmanipulated_loss, manipulated_accuracy = model.evaluate(X_manipulated_test, y_manipulated_test_one_hot, verbose=2)\nprint(f\"Manipulated Test Loss: {manipulated_loss}, Manipulated Test Accuracy: {manipulated_accuracy}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T17:36:26.155775Z","iopub.execute_input":"2024-12-20T17:36:26.156198Z","iopub.status.idle":"2024-12-20T17:37:20.719580Z","shell.execute_reply.started":"2024-12-20T17:36:26.156162Z","shell.execute_reply":"2024-12-20T17:37:20.718493Z"}},"outputs":[{"name":"stdout","text":"Manipulated Test Set Loaded: (6500, 128, 128, 3), (6500,)\n204/204 - 40s - 198ms/step - accuracy: 0.9895 - loss: 0.3466\nManipulated Test Loss: 0.3465944528579712, Manipulated Test Accuracy: 0.9895384907722473\n","output_type":"stream"}],"execution_count":13},{"cell_type":"markdown","source":"# **Evaluating the Model on White-Balanced Test Set**\n\n## **Overview**\nThis section evaluates the model's performance on the white-balanced test set, which has been processed using the Gray World assumption for color normalization. The goal is to determine if white balance correction improves classification accuracy.\n\n## **Steps**\n\n### **1. Load White-Balanced Test Set**\n- The white-balanced dataset is loaded from the specified directory.\n- Images and labels are extracted.\n\n#### **Code**\n```python\nwb_output_dir = \"/kaggle/working/wb_corrected_images\"  # Adjust path as needed\n\n# Load white-balanced test images and labels\nX_wb_test, y_wb_test = load_images_from_directory(wb_output_dir, selected_classes)\nprint(f\"White-Balanced Test Set Loaded: {X_wb_test.shape}, {y_wb_test.shape}\")\n","metadata":{}},{"cell_type":"code","source":"# Load white-balanced test set\nwb_output_dir = \"/kaggle/working/wb_corrected_images\"  # Adjust path as needed\n\n# Load white-balanced test images and labels\nX_wb_test, y_wb_test = load_images_from_directory(wb_output_dir, selected_classes)\n\nprint(f\"White-Balanced Test Set Loaded: {X_wb_test.shape}, {y_wb_test.shape}\")\n\ny_wb_test_one_hot = to_categorical(y_wb_test, num_classes=10)\n\nwb_loss, wb_accuracy = model.evaluate(X_wb_test, y_wb_test_one_hot, verbose=2)\nprint(f\"White-Balanced Test Loss: {wb_loss}, White-Balanced Test Accuracy: {wb_accuracy}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T17:38:14.317278Z","iopub.execute_input":"2024-12-20T17:38:14.317676Z","iopub.status.idle":"2024-12-20T17:39:02.055275Z","shell.execute_reply.started":"2024-12-20T17:38:14.317640Z","shell.execute_reply":"2024-12-20T17:39:02.053977Z"}},"outputs":[{"name":"stdout","text":"White-Balanced Test Set Loaded: (6500, 128, 128, 3), (6500,)\n204/204 - 40s - 195ms/step - accuracy: 0.9966 - loss: 0.3308\nWhite-Balanced Test Loss: 0.33081671595573425, White-Balanced Test Accuracy: 0.9966154098510742\n","output_type":"stream"}],"execution_count":15},{"cell_type":"markdown","source":"# **Evaluating the Model on Original Test Set**\n\n## **Overview**\nIn this step, we evaluate the model's performance on the original (unaltered) test dataset to establish a baseline performance.\n\n\n## **Steps**\n\n### **1. Convert Labels to One-Hot Encoding**\n- Labels are converted into a one-hot encoded format for compatibility with the categorical classification model.\n\n#### **Code**\n```python\n# Convert original test labels to one-hot encoding\ny_test_one_hot = to_categorical(y_test, num_classes=10)\n","metadata":{}},{"cell_type":"code","source":"# Convert original test labels to one-hot encoding\ny_test_one_hot = to_categorical(y_test, num_classes=10)\n\n# Evaluate the model on the original test set\noriginal_loss, original_accuracy = model.evaluate(X_test, y_test_one_hot, verbose=2)\n\n# Print the results\nprint(f\"Original Test Loss: {original_loss}, Original Test Accuracy: {original_accuracy}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T17:40:12.521466Z","iopub.execute_input":"2024-12-20T17:40:12.521937Z","iopub.status.idle":"2024-12-20T17:40:26.393553Z","shell.execute_reply.started":"2024-12-20T17:40:12.521899Z","shell.execute_reply":"2024-12-20T17:40:26.392743Z"}},"outputs":[{"name":"stdout","text":"61/61 - 12s - 195ms/step - accuracy: 0.9492 - loss: 0.4589\nOriginal Test Loss: 0.4589402675628662, Original Test Accuracy: 0.9492307901382446\n","output_type":"stream"}],"execution_count":16},{"cell_type":"markdown","source":"# **Comparison and Reporting of Test Set Performances**\n\n## **Objective**\nThe aim was to evaluate the model's performance on three different test datasets:\n1. **Original Test Set**\n2. **Manipulated Test Set**\n3. **White-Balanced Test Set**\n\n\n## **Results Summary**\n| Test Set                  | Accuracy   | Loss    |\n|---------------------------|------------|---------|\n| **Original Test Set**     | **94.92%** | 0.4589  |\n| **Manipulated Test Set**  | **98.95%** | 0.3466  |\n| **White-Balanced Test Set** | **99.66%** | 0.3308  |\n\n\n## **Insights**\n1. **Original Test Set**:\n   - The model performs well on the original dataset, achieving an accuracy of **94.92%**.\n   - This demonstrates the model's ability to correctly classify data from its training distribution.\n\n2. **Manipulated Test Set**:\n   - With brightness and contrast adjustments, the model achieves an improved accuracy of **98.95%**.\n   - This suggests that the model has learned robust features and generalizes well to visually altered data.\n\n3. **White-Balanced Test Set**:\n   - Applying the Gray World assumption for white balance correction further improves accuracy to **99.66%**.\n   - This shows the effectiveness of normalization in enhancing the model's interpretability of image features.\n\n## **Suggestions for Improvement**\n### **What Could Have Been Done Differently**\nSince I (or we) developed this pipeline, here are a few areas where alternative approaches could have been explored:\n\n1. **Model Enhancements**:\n   - Using pretrained models such as ResNet or EfficientNet could have sped up training and improved results further.\n   - Adding attention mechanisms like SE blocks or CBAM to the model could have refined feature extraction.\n\n2. **Data Preparation**:\n   - Applying more diverse augmentation techniques, such as random noise injection or affine transformations, could have simulated real-world variability better.\n   - Testing with other color correction algorithms (e.g., Histogram Equalization) might have provided additional insights.\n\n3. **Hyperparameter Optimization**:\n   - Performing systematic hyperparameter tuning with libraries like Optuna or HyperOpt might have yielded better configurations.\n   - Experimenting with optimizers like SGD with momentum or adaptive optimizers (e.g., AdamW) could have further fine-tuned performance.\n\n4. **Evaluation**:\n   - Cross-validation could have been used to ensure that the results were consistent across different splits of the dataset.\n   - Testing the model on entirely unseen datasets could have provided a better understanding of its real-world applicability.\n\n### **If the Scores Were Low (<50%)**\nHad the accuracy been significantly lower, here are steps that could have been taken:\n- **Model Simplification**: Reducing overfitting by simplifying the architecture or adding dropout layers.\n- **Augmentation**: Introducing more aggressive augmentations to combat overfitting and improve generalization.\n- **Fine-Tuning**: Training on a smaller dataset using transfer learning could have helped establish a stronger baseline.\n\n## **Conclusion**\nThe model shows exceptional generalization, achieving high accuracy across all test sets. However, improvements in architecture, data preprocessing, and hyperparameter tuning could have potentially pushed the performance even further. The white balance correction proved to be an impactful enhancement, significantly boosting test accuracy. This experiment lays a solid foundation for future iterations of similar tasks.\n","metadata":{}}]}